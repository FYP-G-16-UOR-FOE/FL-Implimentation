{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6AmXMJSK4ir",
        "outputId": "567c02f7-590a-4a48-c2a7-bac27fb26852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # conncet google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3gj1CogJsW4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gzip\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from functools import reduce\n",
        "import math\n",
        "from math import ceil\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. Data Loading (from loadDataSet.ipynb)\n",
        "# -------------------------------------------------------------------\n",
        "# Copy-paste or import this function from your notebook\n",
        "def load_data_from_disk(partition_id: int, only_server_test_data: bool = False):\n",
        "    import torchvision\n",
        "    from torch.utils.data import DataLoader\n",
        "    save_dir = \"/content/drive/MyDrive/client_data_backup2\"\n",
        "    BATCH_SIZE = 64\n",
        "\n",
        "    if only_server_test_data:\n",
        "        testset = torchvision.datasets.CIFAR10(\n",
        "            root='./data', train=False, download=True,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "            ])\n",
        "        )\n",
        "        return DataLoader(testset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "    client_filenames = [\n",
        "        os.path.join(save_dir, f'client_{partition_id}.pt.gz'),\n",
        "        os.path.join(save_dir, f'iid_clients_{partition_id}.pt.gz')\n",
        "    ]\n",
        "    for path in client_filenames:\n",
        "        if os.path.exists(path):\n",
        "            partition_data_path = path\n",
        "            break\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No data file for client {partition_id}\")\n",
        "\n",
        "    with gzip.open(partition_data_path, 'rb') as f:\n",
        "        device_data = torch.load(f, map_location='cpu')\n",
        "    device_data = [(x.to(torch.float32), y) for x,y in device_data]\n",
        "    np.random.shuffle(device_data)\n",
        "    split = int(len(device_data)*0.8)\n",
        "    train_data, test_data = device_data[:split], device_data[split:]\n",
        "\n",
        "    normalize = transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    train_data = [(normalize(x), y) for x,y in train_data]\n",
        "    test_data  = [(normalize(x), y) for x,y in test_data]\n",
        "\n",
        "    trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
        "    valloader  = DataLoader(test_data,  batch_size=BATCH_SIZE, num_workers=1)\n",
        "    return trainloader, valloader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
